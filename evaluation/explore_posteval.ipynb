{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook is to calculate stats about the systems.",
   "id": "f2a0ff0124a1598e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "from evaluation.models import LabelledSuggestion, SuggestionLog\n",
    "from evaluation.utils import (\n",
    "    get_all_suggestions,\n",
    "    get_annotations,\n",
    "    get_experiment_by_id,\n",
    "    get_experiment_infos,\n",
    "    suggestions_are_same,\n",
    ")\n",
    "from experiments.models import ALL_MODELS\n",
    "from experiments.utils import REPO_ROOT\n",
    "from overhearing_agents.utils import read_jsonl\n",
    "\n",
    "experiments = get_experiment_infos()\n",
    "experiments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## get experiment durations",
   "id": "ab41238159d8d9de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for experiment in experiments:\n",
    "    pcm_fp = REPO_ROOT / experiment.pcm_fp\n",
    "    duration = pcm_fp.stat().st_size / 48000\n",
    "    all_suggestions = get_all_suggestions(experiment.id)\n",
    "    print(f\"{experiment.name}: {duration:.2f}s, {len(all_suggestions)} suggestions\")"
   ],
   "id": "e1373be502078bcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# System Timing",
   "id": "fbd07c3012e05088"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\".join(ALL_MODELS))\n",
    "print()\n",
    "print(\"\\n\".join(experiment.id for experiment in experiments))"
   ],
   "id": "ab819d4834c8455b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def system_timing_from_event_logs(system_id: str, experiment_id: str):\n",
    "    experiment = get_experiment_by_id(experiment_id)\n",
    "    experiment_duration = 0\n",
    "\n",
    "    for subdir in (REPO_ROOT / Path(experiment.log_dir)).glob(f\"{system_id}*\"):\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "        if not re.match(rf\"{re.escape(system_id)}(__until-\\d+)?$\", subdir.name):\n",
    "            continue\n",
    "        events_fp = subdir / \"events.jsonl\"\n",
    "        assert events_fp.exists()\n",
    "        events = list(read_jsonl(events_fp))\n",
    "        start = min(e[\"timestamp\"] for e in events)\n",
    "        end = max(e[\"timestamp\"] for e in events)\n",
    "        duration = end - start\n",
    "        experiment_duration += duration\n",
    "    print(system_id, experiment_id, experiment_duration)\n",
    "    return experiment_duration\n",
    "\n",
    "\n",
    "with open(REPO_ROOT / \"data/starless/transcription-timing.json\") as f:\n",
    "    transcript_timing = json.load(f)"
   ],
   "id": "38a4e9251cb6efa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_timing_from_event_logs(\"text.spans\", \"starless-lands-s17\")",
   "id": "59c2cd7ab9f515f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "timings = []\n",
    "for model_id in ALL_MODELS:\n",
    "    model_timing = {\"name\": model_id}\n",
    "    for experiment in experiments:\n",
    "        time_taken = system_timing_from_event_logs(model_id, experiment.id)\n",
    "        if \"text\" in model_id:\n",
    "            time_taken += transcript_timing[experiment.name]\n",
    "        model_timing[experiment.id] = time_taken\n",
    "    timings.append(model_timing)"
   ],
   "id": "790cc591df8482e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "timing_df = pd.DataFrame.from_records(timings)\n",
    "timing_df"
   ],
   "id": "5a7cc21c894fd9ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# metrics",
   "id": "a6be514222215c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Score:\n",
    "    tp: int\n",
    "    fp: int\n",
    "    fn: int\n",
    "\n",
    "    @property\n",
    "    def precision(self):\n",
    "        if self.tp + self.fp == 0:\n",
    "            return 0.0\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    @property\n",
    "    def recall(self):\n",
    "        if self.tp + self.fn == 0:\n",
    "            return 0.0\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    @property\n",
    "    def f1(self):\n",
    "        if self.precision + self.recall == 0:\n",
    "            return 0.0\n",
    "        return (2 * self.precision * self.recall) / (self.precision + self.recall)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.tp=}, {self.fp=}, {self.fn=}, {self.precision=:.3}, {self.recall=:.3}, {self.f1=:.3}\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Score(tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn)\n",
    "\n",
    "    def to_dict(self, key_prefix=\"\"):\n",
    "        data = {**dataclasses.asdict(self), \"precision\": self.precision, \"recall\": self.recall, \"f1\": self.f1}\n",
    "        return {f\"{key_prefix}{k}\": v for k, v in data.items()}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StopwatchSuggestion:\n",
    "    \"\"\"Gold-labelled suggestions from stopwatch annotation\"\"\"\n",
    "\n",
    "    time: float  # the time the annotation happened\n",
    "    matches: list[str]  # the list of strings the suggestion must match to satisfy this label\n",
    "    antimatches: list[str] | None = None  # a list of strings that cannot match\n",
    "\n",
    "\n",
    "def length_aware_ratio(haystack: str, needle: str, **kwargs):\n",
    "    \"\"\"\n",
    "    If the needle is length 1, return 0.\n",
    "    If the needle is longer than the haystack, return full ratio.\n",
    "    Otherwise return partial ratio.\n",
    "    \"\"\"\n",
    "    if len(needle) < 2:\n",
    "        return 0\n",
    "    if len(needle) > len(haystack):\n",
    "        return fuzz.ratio(needle, haystack, **kwargs)\n",
    "    return fuzz.partial_ratio(needle, haystack, **kwargs)\n",
    "\n",
    "\n",
    "def satisfies(prediction: SuggestionLog, label: LabelledSuggestion | StopwatchSuggestion, *, tolerance=300) -> bool:\n",
    "    \"\"\"Whether the given prediction matches the given label.\"\"\"\n",
    "    if isinstance(label, LabelledSuggestion):\n",
    "        if label.score < 0:\n",
    "            return False\n",
    "        return suggestions_are_same(\n",
    "            label.entry,\n",
    "            prediction,\n",
    "            tolerance=tolerance,\n",
    "            npc_speech_similarity_ratio=length_aware_ratio,\n",
    "            npc_speech_similarity_threshold=80,\n",
    "        )\n",
    "    # stopwatch suggestion\n",
    "    timeliness = (label.time - 30) <= prediction.end <= (label.time + tolerance)\n",
    "    string_match = all(s.lower() in str(prediction.suggestion).lower() for s in label.matches)\n",
    "    if label.antimatches:\n",
    "        antimatches = any(s.lower() in str(prediction.suggestion).lower() for s in label.antimatches)\n",
    "    else:\n",
    "        antimatches = False\n",
    "    return timeliness and string_match and not antimatches\n",
    "\n",
    "\n",
    "def get_gold_labels(experiment_id: str) -> list[LabelledSuggestion | StopwatchSuggestion]:\n",
    "    out = []\n",
    "    with open(f\"gold/{experiment_id}.jsonl\") as f:\n",
    "        # with open(f\"annotations/to-dedup-{experiment_id}.jsonl\") as f:  # todo\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "            if \"matches\" in data:\n",
    "                out.append(StopwatchSuggestion(**data))\n",
    "            else:\n",
    "                out.append(LabelledSuggestion.model_validate(data))\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_predictions(experiment_id: str, model_id: str) -> list[SuggestionLog]:\n",
    "    return [p for p in get_all_suggestions(experiment_id) if p.model_key == model_id]\n",
    "\n",
    "\n",
    "def label_is_type(l, suggestion_type):\n",
    "    return (isinstance(l, LabelledSuggestion) and l.entry.suggestion[\"suggest_type\"] == suggestion_type) or (\n",
    "        isinstance(l, StopwatchSuggestion) and suggestion_type in l.matches\n",
    "    )\n",
    "\n",
    "\n",
    "def score(predictions: list[SuggestionLog], gold_labels: list[LabelledSuggestion | StopwatchSuggestion]) -> Score:\n",
    "    \"\"\"\n",
    "    :param predictions:\n",
    "    :param gold_labels: All the positively-annotated labels\n",
    "    \"\"\"\n",
    "\n",
    "    # this block temporarily only considers suggestions up to the latest labelled gold\n",
    "    # latest_gold = max((l.time if isinstance(l, StopwatchSuggestion) else l.entry.end for l in gold_labels), default=0)\n",
    "    # predictions = [p for p in predictions if p.end <= latest_gold]\n",
    "    # end\n",
    "\n",
    "    pos_prediction_idxs = set()\n",
    "    pos_label_idxs = set()\n",
    "\n",
    "    for l_idx, label in enumerate(gold_labels):\n",
    "        for p_idx, prediction in enumerate(predictions):\n",
    "            if p_idx in pos_prediction_idxs:\n",
    "                continue  # each prediction can only be a hit for one label\n",
    "            if satisfies(prediction, label, tolerance=300 if label_is_type(label, \"gamedata\") else 30):\n",
    "                pos_prediction_idxs.add(p_idx)\n",
    "                pos_label_idxs.add(l_idx)\n",
    "                # each label can only be a hit for one prediction unless it's an npc speech\n",
    "                if not label_is_type(label, \"foundry\"):\n",
    "                    break\n",
    "\n",
    "    return Score(\n",
    "        tp=len(pos_prediction_idxs),\n",
    "        fp=len(predictions) - len(pos_prediction_idxs),\n",
    "        fn=len(gold_labels) - len(pos_label_idxs),\n",
    "    )"
   ],
   "id": "f3c490c4ed160312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score(get_predictions(\"starless-lands-s23\", \"openai.audio-zeroshot\"), get_gold_labels(\"starless-lands-s23\"))",
   "id": "fdfda6a13a8af421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sum(1 for e in experiments for l in get_gold_labels(e.id))",
   "id": "30c473928f4b8559",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c9972d066429a5a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def score_all(*, prediction_filter=lambda p: True, label_filter=lambda l: True, col_prefix=\"\", debug=False):\n",
    "    df_data = []\n",
    "    for model_id in ALL_MODELS:\n",
    "        total_score = Score(tp=0, fp=0, fn=0)\n",
    "        for experiment in experiments:\n",
    "            predictions = [p for p in get_predictions(experiment.id, model_id) if prediction_filter(p)]\n",
    "            if not predictions:\n",
    "                continue\n",
    "            labels = [l for l in get_gold_labels(experiment.id) if label_filter(l)]\n",
    "            total_score += score(predictions, labels)\n",
    "        if debug:\n",
    "            print(model_id)\n",
    "            print(total_score)\n",
    "        df_data.append({\"model_id\": model_id, **total_score.to_dict(col_prefix)})\n",
    "    return pd.DataFrame.from_records(df_data, index=\"model_id\")"
   ],
   "id": "f9bf4876d79ecca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agg_df = score_all(debug=True)\n",
    "gamedata_df = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"gamedata\",\n",
    "    label_filter=lambda l: label_is_type(l, \"gamedata\"),\n",
    "    col_prefix=\"gamedata-\",\n",
    "    debug=True,\n",
    ")\n",
    "foundry_df = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"foundry\",\n",
    "    label_filter=lambda l: label_is_type(l, \"foundry\"),\n",
    "    col_prefix=\"foundry-\",\n",
    ")\n",
    "improv_df = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"improvised_npc\",\n",
    "    label_filter=lambda l: label_is_type(l, \"improvised_npc\"),\n",
    "    col_prefix=\"improv-\",\n",
    ")\n",
    "everything_df = agg_df.join([gamedata_df, foundry_df, improv_df])\n",
    "everything_df"
   ],
   "id": "41e8feaeb451bd80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization",
   "id": "5fee1c0499eebf1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f28e5fa648b3e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main Results\n",
    "\n",
    "text vs audio, per model"
   ],
   "id": "5d0c2454d16fe052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pandas code hates automatic formatting\n",
    "# @formatter:off\n",
    "# fmt:off\n",
    "\n",
    "everything_df[\"model_id\"] = [splat[0] for splat in everything_df.index.str.split(\".\")]\n",
    "everything_df[\"modality\"] = [splat[1].split(\"-\")[0] for splat in everything_df.index.str.split(\".\")]\n",
    "everything_df[\"prompt_type\"] = [splat[1].split(\"-\", 1)[1] if \"-\" in splat[1] else splat[1] for splat in everything_df.index.str.split(\".\")]\n",
    "\n",
    "openai_df = everything_df.loc[(everything_df[\"model_id\"].str.contains(\"openai\")) & (everything_df[\"prompt_type\"] == \"zeroshot\")].copy()\n",
    "other_df = everything_df.loc[(~everything_df[\"model_id\"].str.contains(\"openai\")) & (everything_df[\"prompt_type\"] == \"fewshot\") & (everything_df[\"model_id\"] != \"ultravox-tiny\")].copy()\n",
    "baseline_df = everything_df.loc[everything_df[\"model_id\"] == \"text\"].copy()\n",
    "baseline_df[\"modality\"] = \"text\"\n",
    "baseline_df[\"model_id\"] = \"baseline\"\n",
    "new_df = pd.concat([openai_df, other_df, baseline_df], ignore_index=True)\n",
    "\n",
    "melted_f1 = pd.melt(new_df[[\"model_id\", \"f1\", \"gamedata-f1\", \"foundry-f1\", \"improv-f1\", \"modality\"]], id_vars=[\"model_id\", \"modality\"])\n",
    "melted_f1 = melted_f1.rename({\"variable\": \"task\"}, axis=1)\n",
    "melted_f1 = melted_f1.replace({\"f1\": \"overall\", \"gamedata-f1\": \"game\\ndata\", \"foundry-f1\": \"stage\\ndirect\", \"improv-f1\": \"gen\\nnpcs\"})\n",
    "melted_f1[\"model_id\"] = melted_f1[\"model_id\"].str.replace(\"openai\", \"gpt-4o\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=melted_f1,\n",
    "    x=\"task\",\n",
    "    y=\"value\",\n",
    "    col=\"model_id\",\n",
    "    hue=\"modality\",\n",
    "    kind=\"bar\",\n",
    "    height=5,\n",
    "    aspect=0.65,\n",
    "    palette=\"husl\",\n",
    "    legend_out=False,\n",
    ")\n",
    "#sns.displot(data=melted_f1,x=\"model_id\",y=\"value\",hue=\"f1_type\",)\n",
    "\n",
    "# aesthetics\n",
    "sns.set_theme(\"paper\", \"whitegrid\", font=\"serif\", font_scale=1.7)\n",
    "g.set_axis_labels(\"\", \"F1 score\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.tick_params(axis='x')\n",
    "g.figure.subplots_adjust(wspace=0.03, hspace=0)\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(.995, .58), frameon=False)\n",
    "\n",
    "g.savefig(\"figs/performance.pdf\")\n",
    "# @formatter:on\n",
    "# fmt:on"
   ],
   "id": "c2d072b76a43aa93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Input Variation\n",
    "\n",
    "per model, text-noreason/audio-noreason/audio-transcribe"
   ],
   "id": "283df143ef45f651"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "everything_df",
   "id": "69ecff8ad5b46167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pandas code hates automatic formatting\n",
    "# @formatter:off\n",
    "# fmt:off\n",
    "\n",
    "everything_df[\"model_id\"] = [splat[0] for splat in everything_df.index.str.split(\".\")]\n",
    "everything_df[\"variation\"] = [splat[1].replace(\"zeroshot-\", \"\").replace(\"fewshot-\", \"\") for splat in everything_df.index.str.split(\".\")]\n",
    "\n",
    "variations_df = everything_df.loc[everything_df[\"variation\"].str.contains(\"noreason\") | everything_df[\"variation\"].str.contains(\"transcribe\")].copy()\n",
    "main_df = everything_df.loc[\n",
    "    (everything_df[\"model_id\"].str.contains(\"openai\") & everything_df[\"variation\"].str.endswith(\"audio-zeroshot\"))\n",
    "    | (~everything_df[\"model_id\"].str.contains(\"openai\") & everything_df[\"variation\"].str.endswith(\"audio-fewshot\"))\n",
    "].copy()\n",
    "main_df[\"variation\"] = \"audio\"\n",
    "baseline_df = everything_df.loc[everything_df[\"model_id\"] == \"text\"].copy()\n",
    "baseline_df[\"variation\"] = \"baseline\"\n",
    "new_df = pd.concat([variations_df, main_df], ignore_index=True)\n",
    "\n",
    "var_melted_f1 = pd.melt(new_df[[\"model_id\", \"f1\", \"gamedata-f1\", \"foundry-f1\", \"improv-f1\", \"modality\", \"variation\"]], id_vars=[\"model_id\", \"modality\", \"variation\"])\n",
    "var_melted_f1 = var_melted_f1.rename({\"variable\": \"task\"}, axis=1)\n",
    "var_melted_f1 = var_melted_f1.replace({\"f1\": \"overall\", \"gamedata-f1\": \"game\\ndata\", \"foundry-f1\": \"stage\\ndirect\", \"improv-f1\": \"gen\\nnpcs\"})\n",
    "var_melted_f1 = var_melted_f1.loc[var_melted_f1[\"model_id\"] != \"ultravox-tiny\"]\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=var_melted_f1,\n",
    "    x=\"task\",\n",
    "    y=\"value\",\n",
    "    col=\"model_id\",\n",
    "    hue=\"variation\",\n",
    "    kind=\"bar\",\n",
    "    height=5,\n",
    "    aspect=0.65,\n",
    "    palette=\"husl\",\n",
    "    legend_out=False,\n",
    ")\n",
    "#sns.displot(data=melted_f1,x=\"model_id\",y=\"value\",hue=\"f1_type\",)\n",
    "\n",
    "# aesthetics\n",
    "sns.set_theme(\"paper\", \"whitegrid\", font=\"serif\", font_scale=1.7)\n",
    "g.set_axis_labels(\"\", \"F1 score\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.tick_params(axis='x')\n",
    "g.figure.subplots_adjust(wspace=0.03, hspace=0)\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(.995, .58), frameon=False)\n",
    "\n",
    "g.savefig(\"figs/variations.pdf\")\n",
    "# @formatter:on\n",
    "# fmt:on"
   ],
   "id": "8213c91fcef91923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## timing visualization",
   "id": "58911b2050944dec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "core_splits = [\n",
    "    \"openai.text-zeroshot\",\n",
    "    \"openai.audio-zeroshot\",\n",
    "    \"openai-mini.text-zeroshot\",\n",
    "    \"openai-mini.audio-zeroshot\",\n",
    "    \"ultravox.text-fewshot\",\n",
    "    \"ultravox.audio-fewshot\",\n",
    "    \"qwen-25.text-fewshot\",\n",
    "    \"qwen-25.audio-fewshot\",\n",
    "    \"phi-4.text-fewshot\",\n",
    "    \"phi-4.audio-fewshot\",\n",
    "    #'text.spans',\n",
    "]\n",
    "timing_df = pd.read_csv(\"annotations/Starless Lands Data - system timing.csv\")\n",
    "timing_df[\"model_id\"] = [splat[0].replace(\"openai\", \"gpt-4o\") for splat in timing_df[\"name\"].str.split(\".\")]\n",
    "timing_df[\"modality\"] = [\n",
    "    splat[1].replace(\"-zeroshot\", \"\").replace(\"-fewshot\", \"\") for splat in timing_df[\"name\"].str.split(\".\")\n",
    "]\n",
    "timing_df[\"model_key_clean\"] = [splat.replace(\"-zeroshot\", \"\").replace(\"-fewshot\", \"\") for splat in timing_df[\"name\"]]\n",
    "timing_df = timing_df.loc[timing_df[\"name\"].isin(core_splits)]"
   ],
   "id": "a93674c255ef7892",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_theme(\"paper\", \"ticks\", font=\"serif\", font_scale=1.2)\n",
    "g = sns.catplot(\n",
    "    data=timing_df,\n",
    "    x=\"model_id\",\n",
    "    y=timing_df[\"relative\"] - 1,\n",
    "    kind=\"bar\",\n",
    "    hue=\"modality\",\n",
    "    palette=\"husl\",\n",
    "    height=4,\n",
    "    aspect=1.3,\n",
    "    # legend=False,\n",
    ")\n",
    "\n",
    "# g.ax.plot(2.8, 0.57, \"^\", markersize=10, color=\"k\")\n",
    "\n",
    "plt.axhline(0, color=\"k\")\n",
    "# plt.ylim(-0.25, 1.5)\n",
    "g.set_axis_labels(\"\", \"Times Realtime\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.tick_params(axis=\"x\")\n",
    "g.figure.subplots_adjust(wspace=0.03, hspace=0)\n",
    "g.ax.set_yticklabels([f\"{label + 1}x\" for label in g.ax.get_yticks()])\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.2, 0.9), frameon=False)\n",
    "g.savefig(\"figs/timing.pdf\")"
   ],
   "id": "319f1b77704bcbcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## what if we only score recall on stopwatch annotations",
   "id": "72d617b4bb3fda78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agg_df_stopwatch = score_all(label_filter=lambda l: isinstance(l, StopwatchSuggestion))\n",
    "gamedata_df_stopwatch = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"gamedata\",\n",
    "    label_filter=lambda l: label_is_type(l, \"gamedata\") and isinstance(l, StopwatchSuggestion),\n",
    "    col_prefix=\"gamedata-\",\n",
    ")\n",
    "foundry_df_stopwatch = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"foundry\",\n",
    "    label_filter=lambda l: label_is_type(l, \"foundry\") and isinstance(l, StopwatchSuggestion),\n",
    "    col_prefix=\"foundry-\",\n",
    ")\n",
    "\n",
    "improv_df_stopwatch = score_all(\n",
    "    prediction_filter=lambda p: p.suggestion[\"suggest_type\"] == \"improvised_npc\",\n",
    "    label_filter=lambda l: label_is_type(l, \"improvised_npc\") and isinstance(l, StopwatchSuggestion),\n",
    "    col_prefix=\"improv-\",\n",
    ")\n",
    "everything_df_stopwatch = agg_df.join([gamedata_df_stopwatch, foundry_df_stopwatch, improv_df_stopwatch])"
   ],
   "id": "d2ae4ede567d723f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "everything_df_stopwatch",
   "id": "5752285c958d80fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# raw annotations heatmap",
   "id": "7d19b70b9c1b350f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MAIN_MODELS = [\n",
    "    \"openai.audio-zeroshot\",\n",
    "    \"openai.text-zeroshot\",\n",
    "    \"openai-mini.audio-zeroshot\",\n",
    "    \"openai-mini.text-zeroshot\",\n",
    "    \"ultravox.audio-fewshot\",\n",
    "    \"ultravox.text-fewshot\",\n",
    "    \"qwen-25.audio-fewshot\",\n",
    "    \"qwen-25.text-fewshot\",\n",
    "    \"phi-4.audio-fewshot\",\n",
    "    \"phi-4.text-fewshot\",\n",
    "    \"text.spans\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_annotation_index(experiment_id):\n",
    "    annotations = list(get_annotations(experiment_id))\n",
    "    annotations_by_suggestion_id = collections.defaultdict(list)\n",
    "\n",
    "    # go through the human annotations and build up an index\n",
    "    for annotation in annotations:\n",
    "        annotations_by_suggestion_id[annotation.suggestion_id].append(annotation)\n",
    "\n",
    "    # remove all suggestions that are only copies\n",
    "    for suggestion_id, suggestion_annotations in annotations_by_suggestion_id.copy().items():\n",
    "        if all(\"copied\" in a.why for a in suggestion_annotations):\n",
    "            annotations_by_suggestion_id.pop(suggestion_id)\n",
    "\n",
    "    return annotations_by_suggestion_id\n",
    "\n",
    "\n",
    "full_annotation_index = {}\n",
    "for experiment in experiments:\n",
    "    full_annotation_index.update(get_annotation_index(experiment.id))"
   ],
   "id": "f749929eace3ab3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(full_annotation_index)",
   "id": "63a67d9dabee45b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_counts_by_model = collections.Counter()\n",
    "\n",
    "for suggestion_id, annotations in full_annotation_index.items():\n",
    "    labels_merged = {label for annotation in annotations for label in annotation.labels}\n",
    "    model_key = annotations[0].entry.model_key\n",
    "    for label in labels_merged:\n",
    "        label_counts_by_model[(model_key, label)] += 1"
   ],
   "id": "2343deac08f70142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_counts_by_model_records = [\n",
    "    (model_id, label, count)\n",
    "    for (model_id, label), count in label_counts_by_model.items()\n",
    "    if model_id in MAIN_MODELS and label not in (\"appropriate\", \"inappropriate\")\n",
    "]\n",
    "label_counts_df = pd.DataFrame.from_records(label_counts_by_model_records, columns=[\"model_id\", \"label\", \"value\"])"
   ],
   "id": "8933e7a60b367b03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_counts_df_pvt = label_counts_df.pivot(index=\"model_id\", columns=\"label\", values=\"value\")\n",
    "# fill holes\n",
    "label_counts_df_pvt.fillna(0, inplace=True)\n",
    "# normalize\n",
    "label_counts_df_pvt = label_counts_df_pvt.div(label_counts_df_pvt.sum(axis=1), axis=0)\n",
    "label_counts_df_pvt"
   ],
   "id": "9214228aeeeee28e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.heatmap(label_counts_df_pvt)",
   "id": "e877d284f3dbe59c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4242b718053879a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
